{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U_dRQr90uhB"
   },
   "source": [
    "#  Projet NLP : Reconnaissance des Entités Nommées (NER) en langue française\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5e_IMmmzUWL"
   },
   "source": [
    "## 1. Contexte et motivation du projet\n",
    "\n",
    "La croissance exponentielle des données textuelles issues des rapports, articles, réseaux sociaux et documents institutionnels pose un défi majeur en matière d’extraction automatique d’informations pertinentes. Le traitement manuel de ces données est coûteux, chronophage et sujet à des erreurs humaines.\n",
    "\n",
    "Dans ce contexte, la Reconnaissance des Entités Nommées (Named Entity Recognition – NER) constitue une tâche fondamentale du Traitement Automatique du Langage Naturel (NLP). Elle vise à identifier et à classifier automatiquement, au sein d’un texte, des entités sémantiques telles que les noms de personnes, de lieux, d’organisations, de produits, de dates ou de montants.\n",
    "\n",
    "Ce projet s’inscrit dans cette dynamique et vise à concevoir un système automatisé de NER appliqué à des textes en langue française, en s’appuyant sur des données annotées de référence et des modèles modernes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT_TC0_K0ZY8"
   },
   "source": [
    "## 2. Objectifs du projet\n",
    "\n",
    "Les objectifs poursuivis dans ce projet sont les suivants :\n",
    "\n",
    "- Construire un modèle capable d’extraire et de classifier automatiquement les entités nommées présentes dans un texte\n",
    "- Exploiter un corpus annoté riche et complexe afin de couvrir une large diversité d’entités\n",
    "- Comparer différentes approches de modélisation et de vectorisation\n",
    "- Évaluer la performance du modèle à l’aide de métriques adaptées (précision, rappel, F1-score)\n",
    "- Déployer un outil de prédiction sous forme d’API ou d’interface interactive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGCPf-mZ0o2-"
   },
   "source": [
    "## 3. Présentation du jeu de données\n",
    "\n",
    "### 3.1 Description générale\n",
    "\n",
    "Le projet repose sur un corpus annoté au format CoNLL, issu du dataset MultiCoNER v2 pour la langue française. Ce corpus se distingue par la richesse et la diversité de ses annotations, incluant des entités fines, ambiguës et dépendantes du contexte.\n",
    "\n",
    "Les annotations suivent le schéma standard BIO :\n",
    "- B-XXX : début d’une entité nommée\n",
    "- I-XXX : continuation de l’entité\n",
    "- O : absence d’entité\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZZ3S5-y0qbV"
   },
   "source": [
    "### 3.2 Organisation des données\n",
    "\n",
    "Les données sont organisées selon une partition classique en apprentissage automatique :\n",
    "\n",
    "- Ensemble d’entraînement (train) : utilisé pour l’apprentissage du modèle\n",
    "- Ensemble de validation (dev) : utilisé pour l’ajustement des hyperparamètres\n",
    "- Ensemble de test (test) : utilisé pour l’évaluation finale des performances\n",
    "\n",
    "Chaque ensemble est stocké dans un fichier distinct au format .conll.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vQSB5M20rkJ"
   },
   "source": [
    "## 4. Chargement des données\n",
    "\n",
    "> Les fichiers CoNLL sont déjà disponibles localement dans le dossier `data/` du repository. On utilise donc des chemins relatifs pour garantir la reproductibilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpdycMt3xXF0",
    "outputId": "a9615f22-d202-459a-c563-656aabb58248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\fr_train.conll -> True\n",
      "..\\data\\fr_dev.conll -> True\n",
      "..\\data\\fr_test.conll -> True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "train_path = DATA_DIR / \"fr_train.conll\"\n",
    "dev_path   = DATA_DIR / \"fr_dev.conll\"\n",
    "test_path  = DATA_DIR / \"fr_test.conll\"\n",
    "\n",
    "for p in [train_path, dev_path, test_path]:\n",
    "    print(p, \"->\", p.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtyqSi4g09JL"
   },
   "source": [
    "## 5. Lecture et structuration des fichiers CoNLL\n",
    "\n",
    "Afin de rendre les fichiers CoNLL exploitables par les modèles de traitement automatique du langage, une fonction de lecture est définie pour reconstruire les phrases et associer chaque token à son étiquette correspondante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3c4rBVS0yliF"
   },
   "outputs": [],
   "source": [
    "def read_conll(path):\n",
    "    \"\"\"Lecture d'un fichier CoNLL (BIO).\"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = []\n",
    "    sentence_labels = []\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # fin de phrase\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(sentence_labels)\n",
    "                    sentence = []\n",
    "                    sentence_labels = []\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            token = parts[0]\n",
    "            ner_tag = parts[-1]\n",
    "\n",
    "            sentence.append(token)\n",
    "            sentence_labels.append(ner_tag)\n",
    "\n",
    "    # flush final\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(sentence_labels)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fT4wRmG1Ip4"
   },
   "source": [
    "## 6. Chargement des ensembles d’entraînement, de validation et de test\n",
    "\n",
    "Les fichiers sont ensuite chargés en mémoire et séparés selon leur rôle dans le processus d’apprentissage et d’évaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bXbAGHdlxZOe"
   },
   "outputs": [],
   "source": [
    "data_train = train_path\n",
    "data_dev   = dev_path\n",
    "data_test  = test_path\n",
    "\n",
    "train_sent, train_labels = read_conll(data_train)\n",
    "dev_sent, dev_labels     = read_conll(data_dev)\n",
    "test_sent, test_labels   = read_conll(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8l8GXyw1MZj"
   },
   "source": [
    "## 7. Vérification de l’intégrité des données\n",
    "\n",
    "Avant de procéder à la phase de modélisation, une vérification est effectuée afin de s’assurer de la cohérence des données chargées et de la bonne reconstruction des phrases et des annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA1iDpZAxZIS",
    "outputId": "bbbc5a3f-bc92-40fc-ad39-09079718a2c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 16548\n",
      "Dev  : 857\n",
      "Test : 249786\n",
      "['#', 'elle', 'porte', 'le', 'nom', 'de', 'la', 'romancière', 'américaine', 'susan', 'sontag', '(', '1933', '2004', ')', '.']\n",
      "['domain=fr', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Artist', 'I-Artist', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", len(train_sent))\n",
    "print(\"Dev  :\", len(dev_sent))\n",
    "print(\"Test :\", len(test_sent))\n",
    "\n",
    "print(train_sent[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI8bJgcW1Xq4"
   },
   "source": [
    "## 8. Modélisation et évaluation\n",
    "\n",
    "> Les étapes suivantes couvrent l'analyse exploratoire, deux modèles (baseline et CRF), l'évaluation et la sauvegarde du modèle pour l'API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Analyse exploratoire des labels\n",
    "\n",
    "> Distribution des étiquettes BIO dans l'ensemble d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb labels: 68\n",
      "Top 10 labels: [('O', 196008), ('domain=fr', 16548), ('I-Artist', 3787), ('B-Artist', 3634), ('I-VisualWork', 3608), ('B-HumanSettlement', 2932), ('B-WrittenWork', 2268), ('I-OtherPER', 2176), ('B-OtherPER', 1874), ('B-VisualWork', 1794)]\n",
      "Exemple labels: ['B-AerospaceManufacturer', 'B-AnatomicalStructure', 'B-ArtWork', 'B-Artist', 'B-Athlete', 'B-CarManufacturer', 'B-Cleric', 'B-Clothing', 'B-Disease', 'B-Drink', 'B-Facility', 'B-Food', 'B-HumanSettlement', 'B-MedicalProcedure', 'B-Medication/Vaccine', 'B-MusicalGRP', 'B-MusicalWork', 'B-ORG', 'B-OtherLOC', 'B-OtherPER']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter([l for labs in train_labels for l in labs])\n",
    "print(\"Nb labels:\", len(label_counts))\n",
    "print(\"Top 10 labels:\", label_counts.most_common(10))\n",
    "\n",
    "unique_labels = sorted(label_counts.keys())\n",
    "print(\"Exemple labels:\", unique_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Baseline : Logistic Regression (token-level)\n",
    "\n",
    "> Modèle simple pour établir un point de comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET STATISTICS\n",
      "============================================================\n",
      "Train: 16,548 phrases, 264,291 tokens\n",
      "Dev:   857 phrases, 13,919 tokens\n",
      "============================================================\n",
      "\n",
      "[1/3] Extracting features...\n",
      "✓ Features extracted in 12.62s (264,291 samples)\n",
      "\n",
      "[2/3] Training Logistic Regression...\n",
      "     (Using: solver='saga', max_iter=100, tol=1e-2 for speed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive\\Bureau\\NLP\\ner-multiconer\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\hp\\OneDrive\\Bureau\\NLP\\ner-multiconer\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model trained in 681.92s\n",
      "\n",
      "[3/3] Predicting and evaluating...\n",
      "✓ Predictions made in 4.38s\n",
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive\\Bureau\\NLP\\ner-multiconer\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: domain=fr seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (dev): 0.22498667140572243\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "AerospaceManufacturer       0.12      0.91      0.21        11\n",
      "  AnatomicalStructure       0.06      0.47      0.10        15\n",
      "              ArtWork       0.01      0.23      0.02        13\n",
      "               Artist       0.05      0.11      0.07       189\n",
      "              Athlete       0.10      0.29      0.15        72\n",
      "      CarManufacturer       0.09      0.54      0.15        13\n",
      "               Cleric       0.09      0.30      0.14        20\n",
      "             Clothing       0.01      0.27      0.02        11\n",
      "              Disease       0.02      0.44      0.04        16\n",
      "                Drink       0.04      0.36      0.07        11\n",
      "             Facility       0.02      0.22      0.03        54\n",
      "                 Food       0.01      0.38      0.02        13\n",
      "      HumanSettlement       0.24      0.48      0.32       155\n",
      "     MedicalProcedure       0.01      0.45      0.02        11\n",
      "   Medication/Vaccine       0.03      0.58      0.06        12\n",
      "           MusicalGRP       0.05      0.37      0.09        30\n",
      "          MusicalWork       0.02      0.15      0.04        27\n",
      "                  ORG       0.10      0.26      0.15        73\n",
      "             OtherLOC       0.02      0.23      0.03        13\n",
      "             OtherPER       0.11      0.11      0.11       101\n",
      "            OtherPROD       0.08      0.31      0.13        45\n",
      "           Politician       0.03      0.01      0.02        67\n",
      "          PrivateCorp       0.53      0.91      0.67        11\n",
      "           PublicCorp       0.08      0.68      0.14        22\n",
      "            Scientist       0.07      0.21      0.10        19\n",
      "             Software       0.03      0.43      0.06        23\n",
      "            SportsGRP       0.05      0.17      0.08        35\n",
      "        SportsManager       0.01      0.20      0.03        15\n",
      "              Station       0.08      0.20      0.11        20\n",
      "              Symptom       0.04      0.50      0.08        10\n",
      "              Vehicle       0.01      0.05      0.02        20\n",
      "           VisualWork       0.02      0.14      0.03        88\n",
      "          WrittenWork       0.09      0.73      0.17       117\n",
      "             omain=fr       1.00      1.00      1.00       857\n",
      "\n",
      "            micro avg       0.14      0.57      0.22      2209\n",
      "            macro avg       0.10      0.37      0.13      2209\n",
      "         weighted avg       0.44      0.57      0.46      2209\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from src.features import sent2features\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "def predict_logreg(model, sentences):\n",
    "    all_feats = [feat for sent in sentences for feat in sent2features(sent)]\n",
    "    all_preds = model.predict(all_feats)\n",
    "\n",
    "    preds = []\n",
    "    idx = 0\n",
    "    for sent in sentences:\n",
    "        preds.append(list(all_preds[idx : idx + len(sent)]))\n",
    "        idx += len(sent)\n",
    "    return preds\n",
    "\n",
    "# Stats du dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "total_tokens_train = sum(len(s) for s in train_sent)\n",
    "total_tokens_dev = sum(len(s) for s in dev_sent)\n",
    "print(f\"Train: {len(train_sent):,} phrases, {total_tokens_train:,} tokens\")\n",
    "print(f\"Dev:   {len(dev_sent):,} phrases, {total_tokens_dev:,} tokens\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1/3] Extracting features...\")\n",
    "start = time.time()\n",
    "X_train = [feat for sent in train_sent for feat in sent2features(sent)]\n",
    "y_train = [lab for labs in train_labels for lab in labs]\n",
    "feat_time = time.time() - start\n",
    "print(f\"✓ Features extracted in {feat_time:.2f}s ({len(X_train):,} samples)\")\n",
    "\n",
    "print(\"\\n[2/3] Training Logistic Regression...\")\n",
    "print(\"     (Using: solver='saga', max_iter=100, tol=1e-2 for speed)\")\n",
    "start = time.time()\n",
    "\n",
    "baseline = Pipeline(\n",
    "    [\n",
    "        (\"vec\", DictVectorizer(sparse=True)),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                solver=\"saga\",\n",
    "                max_iter=100,\n",
    "                tol=1e-2,\n",
    "                n_jobs=-1,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "train_time = time.time() - start\n",
    "print(f\"✓ Model trained in {train_time:.2f}s\")\n",
    "\n",
    "print(\"\\n[3/3] Predicting and evaluating...\")\n",
    "start = time.time()\n",
    "preds_dev = predict_logreg(baseline, dev_sent)\n",
    "pred_time = time.time() - start\n",
    "print(f\"✓ Predictions made in {pred_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"F1 (dev):\", f1_score(dev_labels, preds_dev))\n",
    "print(classification_report(dev_labels, preds_dev))\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Modèle séquentiel : CRF\n",
    "\n",
    "> Le CRF capture les dépendances entre étiquettes successives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (dev): 0.7211281303185024\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "AerospaceManufacturer       0.78      0.64      0.70        11\n",
      "  AnatomicalStructure       0.86      0.40      0.55        15\n",
      "              ArtWork       0.67      0.15      0.25        13\n",
      "               Artist       0.55      0.60      0.57       189\n",
      "              Athlete       0.39      0.42      0.41        72\n",
      "      CarManufacturer       0.80      0.62      0.70        13\n",
      "               Cleric       0.50      0.30      0.37        20\n",
      "             Clothing       1.00      0.27      0.43        11\n",
      "              Disease       0.57      0.25      0.35        16\n",
      "                Drink       0.67      0.36      0.47        11\n",
      "             Facility       0.71      0.56      0.63        54\n",
      "                 Food       0.75      0.46      0.57        13\n",
      "      HumanSettlement       0.76      0.64      0.69       155\n",
      "     MedicalProcedure       0.83      0.45      0.59        11\n",
      "   Medication/Vaccine       0.88      0.58      0.70        12\n",
      "           MusicalGRP       0.71      0.33      0.45        30\n",
      "          MusicalWork       0.33      0.19      0.24        27\n",
      "                  ORG       0.72      0.42      0.53        73\n",
      "             OtherLOC       0.67      0.46      0.55        13\n",
      "             OtherPER       0.45      0.38      0.41       101\n",
      "            OtherPROD       0.62      0.36      0.45        45\n",
      "           Politician       0.28      0.19      0.23        67\n",
      "          PrivateCorp       1.00      0.82      0.90        11\n",
      "           PublicCorp       0.59      0.59      0.59        22\n",
      "            Scientist       0.67      0.32      0.43        19\n",
      "             Software       0.56      0.39      0.46        23\n",
      "            SportsGRP       0.71      0.57      0.63        35\n",
      "        SportsManager       0.50      0.20      0.29        15\n",
      "              Station       0.56      0.50      0.53        20\n",
      "              Symptom       0.80      0.40      0.53        10\n",
      "              Vehicle       0.57      0.20      0.30        20\n",
      "           VisualWork       0.55      0.48      0.51        88\n",
      "          WrittenWork       0.74      0.49      0.59       117\n",
      "             omain=fr       1.00      1.00      1.00       857\n",
      "\n",
      "            micro avg       0.78      0.67      0.72      2209\n",
      "            macro avg       0.67      0.44      0.52      2209\n",
      "         weighted avg       0.76      0.67      0.70      2209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "X_train_seq = [sent2features(sent) for sent in train_sent]\n",
    "X_dev_seq   = [sent2features(sent) for sent in dev_sent]\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm=\"lbfgs\",\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    " )\n",
    "\n",
    "crf.fit(X_train_seq, train_labels)\n",
    "\n",
    "preds_dev = crf.predict(X_dev_seq)\n",
    "print(\"F1 (dev):\", f1_score(dev_labels, preds_dev))\n",
    "print(classification_report(dev_labels, preds_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Sauvegarde du modèle (pour l'API)\n",
    "\n",
    "> Le modèle sauvegardé sera utilisé par l'API FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ..\\models\\ner_model.joblib\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "MODEL_DIR = Path(\"..\") / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(crf, MODEL_DIR / \"ner_model.joblib\")\n",
    "print(\"Model saved to\", MODEL_DIR / \"ner_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Évaluation sur test\n",
    "\n",
    "> Performance finale sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_sent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_test = [sent2features(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_sent\u001b[49m]\n\u001b[32m      2\u001b[39m preds_test = crf.predict(X_test)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== TEST SET ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_sent' is not defined"
     ]
    }
   ],
   "source": [
    "X_test = [sent2features(sent) for sent in test_sent]\n",
    "preds_test = crf.predict(X_test)\n",
    "\n",
    "print(\"=== TEST SET ===\")\n",
    "print(\"F1:\", f1_score(test_labels, preds_test))\n",
    "print(classification_report(test_labels, preds_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
